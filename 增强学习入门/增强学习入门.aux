\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}介绍}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}增强学习}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}例子}{3}{subsection.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}增强学习的元素}{4}{subsection.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}局限性和展望}{5}{subsection.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}详细的案例：Tic-Tac-Toe}{6}{subsection.1.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Tic-Tac-Toe游戏的走子序列。黑色实线代表游戏中的实际走子；虚线代表候选的走子方案。而第二步我们执行的是探索走子，也就是说存在一个走子方案$e^*$的值更高。探索性走子不会促使任何学习行为的产生，但是后续的走子可以辅助值方程的更新，如图中红色箭头所示\relax }}{7}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{f1_1}{{1}{7}{Tic-Tac-Toe游戏的走子序列。黑色实线代表游戏中的实际走子；虚线代表候选的走子方案。而第二步我们执行的是探索走子，也就是说存在一个走子方案$e^*$的值更高。探索性走子不会促使任何学习行为的产生，但是后续的走子可以辅助值方程的更新，如图中红色箭头所示\relax \relax }{figure.caption.1}{}}
\newlabel{e_1}{{1}{7}{详细的案例：Tic-Tac-Toe\relax }{equation.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}总结}{9}{subsection.1.6}}
\@writefile{toc}{\contentsline {section}{\numberline {2}多臂老虎机游戏}{9}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}多臂赌博机问题}{10}{subsection.2.1}}
\newlabel{e_2}{{2}{10}{多臂赌博机问题\relax }{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}动作-值（Action-Value）方法}{11}{subsection.2.2}}
\newlabel{e_3}{{3}{11}{动作-值（Action-Value）方法\relax }{equation.2.3}{}}
\newlabel{e_4}{{4}{11}{动作-值（Action-Value）方法\relax }{equation.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces 10臂赌博机上的一个简单案例。十个不同拉杆对应的$q_*(a)$的真实值是从一个标准正态分布中所获取的，而拉杆对应的奖励则来自于以$q_*(a)$为均值，方差为1 的正态分布，也就是图中所示的灰色分布\relax }}{12}{figure.caption.2}}
\newlabel{f2_1}{{2}{12}{10臂赌博机上的一个简单案例。十个不同拉杆对应的$q_*(a)$的真实值是从一个标准正态分布中所获取的，而拉杆对应的奖励则来自于以$q_*(a)$为均值，方差为1 的正态分布，也就是图中所示的灰色分布\relax \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}10臂赌博机测试}{12}{subsection.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces 两千个回合中的贪心算法，$\epsilon $-贪心算法的平均性能对比。所有的方法都使用样本平均的方法来对值方程进行估计\relax }}{13}{figure.caption.3}}
\newlabel{f2_2}{{3}{13}{两千个回合中的贪心算法，$\epsilon $-贪心算法的平均性能对比。所有的方法都使用样本平均的方法来对值方程进行估计\relax \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}进阶讨论}{13}{subsection.2.4}}
\newlabel{e_5}{{5}{13}{进阶讨论\relax }{equation.2.5}{}}
\newlabel{e_6}{{6}{13}{进阶讨论\relax }{equation.2.6}{}}
\newlabel{e_7}{{7}{13}{进阶讨论\relax }{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}处理非稳定性问题}{14}{subsection.2.5}}
\newlabel{e2.5}{{9}{14}{处理非稳定性问题\relax }{equation.2.9}{}}
\newlabel{e2.6}{{10}{14}{处理非稳定性问题\relax }{equation.2.10}{}}
\newlabel{e2.7}{{11}{14}{处理非稳定性问题\relax }{equation.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces 在10臂赌博机上正向初始值贪心方法和$\epsilon $-贪心方法的效果对比，这两种方法都采用固定步长，$\alpha =0.1$\relax }}{15}{figure.caption.4}}
\newlabel{f2_3}{{4}{15}{在10臂赌博机上正向初始值贪心方法和$\epsilon $-贪心方法的效果对比，这两种方法都采用固定步长，$\alpha =0.1$\relax \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}最优初始值}{15}{subsection.2.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces UCB方法在10臂赌博机进行试验的平均结果。可以看出，除了在最开始的$k$个阶段，它需要尝试不同的行动方案，所以性能不太好，在后期的性能要优于$\epsilon $-贪心方法\relax }}{16}{figure.caption.5}}
\newlabel{f2_4}{{5}{16}{UCB方法在10臂赌博机进行试验的平均结果。可以看出，除了在最开始的$k$个阶段，它需要尝试不同的行动方案，所以性能不太好，在后期的性能要优于$\epsilon $-贪心方法\relax \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}基于置信上界的行动选择方法}{16}{subsection.2.7}}
\newlabel{e2.10}{{12}{16}{基于置信上界的行动选择方法\relax }{equation.2.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces 梯度赌博算法在考虑基准线和不考虑基准线两种情况下的平均性能\relax }}{17}{figure.caption.6}}
\newlabel{f2_5}{{6}{17}{梯度赌博算法在考虑基准线和不考虑基准线两种情况下的平均性能\relax \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}梯度赌博方法}{17}{subsection.2.8}}
\newlabel{e2.11}{{13}{17}{梯度赌博方法\relax }{equation.2.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9}耦合搜索（时序依赖的赌博机）}{18}{subsection.2.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10}总结}{18}{subsection.2.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces 本章所讨论的各种赌博机算法的参数研究，每一个点上的值都是特定的算法在特定的参数设置下1000个轮次奖励的平均\relax }}{19}{figure.caption.7}}
\newlabel{f2_6}{{7}{19}{本章所讨论的各种赌博机算法的参数研究，每一个点上的值都是特定的算法在特定的参数设置下1000个轮次奖励的平均\relax \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}有限马尔科夫决策过程}{19}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces 在马尔科夫决策过程中Agent-Environment之间的交互\relax }}{20}{figure.caption.8}}
\newlabel{f3_1}{{8}{20}{在马尔科夫决策过程中Agent-Environment之间的交互\relax \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}学习机-环境交互接口}{20}{subsection.3.1}}
\newlabel{e3.1}{{15}{20}{学习机-环境交互接口\relax }{equation.3.15}{}}
\newlabel{e3.2}{{16}{20}{学习机-环境交互接口\relax }{equation.3.16}{}}
\newlabel{e3.3}{{17}{20}{学习机-环境交互接口\relax }{equation.3.17}{}}
\newlabel{e3.4}{{18}{21}{学习机-环境交互接口\relax }{equation.3.18}{}}
\newlabel{e3.5}{{19}{21}{学习机-环境交互接口\relax }{equation.3.19}{}}
\newlabel{e3.6}{{20}{21}{学习机-环境交互接口\relax }{equation.3.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}目标和奖励}{22}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}回报和回合}{23}{subsection.3.3}}
\newlabel{e3.7}{{21}{23}{回报和回合\relax }{equation.3.21}{}}
\newlabel{e3.8}{{22}{23}{回报和回合\relax }{equation.3.22}{}}
\newlabel{e3.9}{{23}{24}{回报和回合\relax }{equation.3.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}回合制和持续制任务的统一符号}{24}{subsection.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces 回合制任务在最后进入了自吸收状态\relax }}{24}{figure.caption.9}}
\newlabel{f3_1_1}{{9}{24}{回合制任务在最后进入了自吸收状态\relax \relax }{figure.caption.9}{}}
\newlabel{e3.11}{{24}{24}{回合制和持续制任务的统一符号\relax }{equation.3.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}策略和值方程}{24}{subsection.3.5}}
\newlabel{e3.12}{{25}{25}{策略和值方程\relax }{equation.3.25}{}}
\newlabel{e3.13}{{26}{25}{策略和值方程\relax }{equation.3.26}{}}
\newlabel{e3.14}{{27}{25}{策略和值方程\relax }{equation.3.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces 值方程的后向传播图\relax }}{26}{figure.caption.10}}
\newlabel{f3_1_2}{{10}{26}{值方程的后向传播图\relax \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}最优策略和最优值方程}{26}{subsection.3.6}}
\newlabel{e3.15}{{28}{26}{最优策略和最优值方程\relax }{equation.3.28}{}}
\newlabel{e3.16}{{29}{26}{最优策略和最优值方程\relax }{equation.3.29}{}}
\newlabel{e3.17}{{30}{26}{最优策略和最优值方程\relax }{equation.3.30}{}}
\newlabel{e3.20}{{32}{27}{最优策略和最优值方程\relax }{equation.3.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces $v_*$和$q_*$的Bellman最优等式的计算过程\relax }}{27}{figure.caption.11}}
\newlabel{f3_4}{{11}{27}{$v_*$和$q_*$的Bellman最优等式的计算过程\relax \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}最优化和近似}{28}{subsection.3.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}总结}{28}{subsection.3.8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}动态规划}{29}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}策略估计（预测）}{30}{subsection.4.1}}
\newlabel{e4.4}{{33}{30}{策略估计（预测）\relax }{equation.4.33}{}}
\newlabel{e4.5}{{34}{30}{策略估计（预测）\relax }{equation.4.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}策略改进}{31}{subsection.4.2}}
\newlabel{e4.6}{{35}{31}{策略改进\relax }{equation.4.35}{}}
\newlabel{e4.7}{{36}{31}{策略改进\relax }{equation.4.36}{}}
\newlabel{e4.8}{{37}{31}{策略改进\relax }{equation.4.37}{}}
\newlabel{e4.9}{{38}{32}{策略改进\relax }{equation.4.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}策略迭代}{32}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}值循环方法}{33}{subsection.4.4}}
\newlabel{e4.10}{{41}{33}{值循环方法\relax }{equation.4.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}异步动态规划}{34}{subsection.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}广义策略迭代}{34}{subsection.4.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}动态规划的效率}{35}{subsection.4.7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}蒙特卡洛方法}{35}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}蒙特卡洛预测}{36}{subsection.5.1}}
